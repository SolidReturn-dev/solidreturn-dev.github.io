<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]>-->
<html lang="en">
<!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <!--[if IE]>
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <![endif]-->
    <title> Data driven APPs : solidreturn.dev home</title>
    <!--REQUIRED STYLE SHEETS-->
    <!-- BOOTSTRAP CORE STYLE CSS -->
    <link href="/assets/css/bootstrap.css" rel="stylesheet" />
    <!-- FONTAWESOME STYLE CSS -->
    <link href="/assets/css/font-awesome.min.css" rel="stylesheet" />
      <!-- VEGAS STYLE CSS -->
    <link href="/assets/css/vegas.min.css" rel="stylesheet">
    <!-- link rel="stylesheet" href="https://jaysalvat.github.io/vegas/releases/latest/vegas.min.css" -->
    <!-- CUSTOM STYLE CSS -->
    <link href="/assets/css/style.css" rel="stylesheet" />
    <!-- GOOGLE FONT -->
    <link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->

</head>
<body >
     <div class="navbar navbar-inverse navbar-fixed-top scrollclass" >
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand" href="#"><img src="/assets/img/head_logo.png" alt="" class="user-img img-logo image-responsive" /></a>

                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                
            </div>
            <div class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="/index.html#home">HOME</a></li>
                    <li><a href="/index.html#services">SERVICES</a></li>
                    <li><a href="/index.html#BLOG">BLOG</a></li>
                    <li><a href="/index.html#Workshops">WORKSHOPS</a></li>
                    <li><a href="/index.html#technology">TECHNOLOGIES</a></li>
                    <li><a href="/index.html#projects">PROJECTS</a></li>
                    <li><a href="/index.html#contact">CONTACT</a></li>
                    <li><a href="/index.html#home">EN</a></li>
                </ul>
            </div>
           
        </div>
    </div>
   
    
   
    <!--HOME SECTION-->
    <div class="container">
    <div class="for-full-back color-white" id="home">
        <div class="row upspace">
            <div class="col-md-1"></div>
            <div class="col-md-11">
                <h1 class="head-main text-center">Amplifing the power of large language models (LLMs)</h1>
                <h2 class="head-last text-center"><b>9/03/2024</b></h2>
            
                <h2 class="head-last text-justify"><b>Introduction</b></h2>
                <h4 class="head-last text-justify">
                    <p>
                        In the vast expanse of burgeoning capabilities, tools emerge as the unsung heroes, 
                        guiding not only humans but also other creatures. Bestowing the power to transcend 
                        the limitations of their physical forms, these tools orchestrate a symphony of interaction 
                        with the ever-changing environment. A recent awakening has ignited fervor in the quest 
                        to amplify the prowess of the mighty large language models (LLMs). The mission is to adorn 
                        them with instruments that break free from the shackles of static knowledge and the 
                        customary text-in, text-out interface. The grand objective: to bestow upon these models 
                        the dynamic prowess to seize real-time knowledge, commune with external realms of reason, 
                        and execute resounding feats in the expansive tapestry beyond.
                    </p><p>
                        Ventures into the realm of tool-augmented LLMs revolve around a noble cause—simplifying the integration 
                        of tools or enhancing the mastery to access an abundance of tools, a realm exemplified by the staggering 
                        potential of unleashing up to 16,000 APIs. These quests predominantly follow two venerable paths: 
                        <ol>
                            <li>
                                1) The saga of In-Context Learning (ICL), where frozen LLMs are beckoned with the whispers of API 
                        specifications and tales of tool usage (instruction-API call pairs), and 
                            </li>
                            <li>
                                2) the epic journey of fine-tuning, where LLMs craft tool use examples, forging them into a potent 
                        weapon.
                            </li>
                        </ol>
                    </p>
                </h4>
                <!-- a href="#about" ><img src="assets/img/logo.png" alt="" class="img-center image-responsive"/></a -->
            </div>
            <div class="col-md-6 col-md-offset-3 slide-custom">
                <img src="/assets/img/blog/tools-LLM-1.png" alt="" class="img-c"  />
                    
            </div>
        </div>
        <div class="row">
            <div class="col-md-12">
                <h1 class="head-last text-justify"><b>The untold saga</b></h1>
                <h4 class="head-last text-justify">
                    <p>
                        In the realm of honing mastery and wielding tools with unrivaled finesse, there lies 
                        an untold saga, a tale overshadowed by the quest for extensive coverage and 
                        flexibility. This hidden facet, obscured in the shadows, is none other than 
                        the accuracy with which a Large Language Model (LLM) channels the essence of the 
                        tools it has been forged with. While the echoes of In-Context Learning (ICL) 
                        reverberate with flexibility, the elusive pinnacle of production-level accuracy 
                        remains shrouded in the mists of challenge. Fine-tuning, a noble endeavor with the 
                        promise of heightened accuracy through an arsenal of examples, has, alas, directed 
                        its gaze toward the uncharted realms of unseen tools, forsaking the optimization of 
                        an LLM's prowess in wielding the tools it encountered during its training odyssey. 
                        In the crucible of reality, where tool-augmented LLMs embark on journeys fraught 
                        with consequential actions—be it navigating the intricate dance of financial 
                        transactions or executing legally binding operations—a clarion call for accuracy 
                        echoes through the corridors. The specter of inaccurate tool usage, a looming nemesis, 
                        threatens to unfurl undesirable and harmful outcomes, casting shadows that swiftly 
                        erode the sacred trust bestowed upon these majestic models.
                    </p><p>
                        To truly unravel the secrets of tool mastery, the quest turns to the sacred annals 
                        of biological wisdom, where humans, apes, and corvids etch their heroic tales. The 
                        profound cognitive odyssey of learning to wield a tool unfolds through the tapestry 
                        of diverse cognitive processes.
                    </p>
                    
                </h4>
                <!-- a href="#about" ><img src="assets/img/logo.png" alt="" class="img-center image-responsive"/></a -->
            </div>
            <div class="col-md-6 col-md-offset-3 slide-custom">
                <img src="/assets/img/blog/tools-LLM-2.png" alt="" class="img-c"  />
                    
            </div>
        </div>
        <div class="row">
        
            <div class="col-md-12">
                <h2 class="head-last text-justify"><b>The melody of trial and error</b></h2>
                <h4 class="head-last text-justify">
                    <p>
                        At its core, the melody of trial and error resonates as a symphony in the grand opera of tool learning. 
                        The artistry of mastering a tool transcends the mere recitation of its user manual; instead, it calls for 
                        an exploration of myriad methods, an observance of outcomes, and a dance with the dichotomy of success 
                        and failure. Intelligent beings, akin to celestial artists, weave not only the threads of trial and 
                        error but also actively conjure visions, imagining and simulating plausible scenarios beyond the realms 
                        of perception.
                    </p>
                    <p>
                        Behold, the symphony of remembrance, an epic tale woven by the threads of both fleeting whispers and 
                        enduring echoes, resonates across the annals of time. In the grand tapestry of knowledge, both the 
                        ephemeral dance of short-term memory and the enduring saga of long-term recollection orchestrate a 
                        pivotal melody, guiding the seekers of wisdom through the realms of progressive learning and the 
                        cyclical embrace of tools\.
                    </p>
                    <p>
                        Amidst the cosmic challenges that echo through the celestial corridors, a beacon of hope emerges—Simulated
                         Trial and Error (STE), a mythical approach inspired by the very essence of biological mechanisms. 
                         This mystical quest seeks to empower the majestic beings 
                         known as Large Language Models (LLMs) with the prowess of tools. When presented with the arcane 
                         artifact of a tool, an API clad in enigmatic specifications, STE calls forth the LLM to embark on a 
                         journey of simulation and imagination. The ethereal landscapes of plausible scenarios unfold, birthing 
                         instructions that unveil the arcane secrets of wielding the tool.
                    </p>
                    <p>
                        A celestial dance commences—a ballet of iterative interactions with the API, a symphony of synthesized 
                        instructions echoing through the cosmic expanse. The vigilant observers, guardians of the ongoing trial, 
                        stand witness to the cosmic performance. To enrich the tapestry of simulated instructions, memory 
                        mechanisms take root. A short-term memory, akin to the comet's trail in the cosmic canvas, captures 
                        recent trajectories of trial and error, guiding the seeker to profound depths within a single episode. 
                        In tandem, a long-term memory, an ancient tome imbued with distilled wisdom from past explorations and 
                        reflections, becomes the guiding star, steering the course of progressive learning over the epochs.
                    </p>
                    <img src="assets/img/blog/tools-LLM-3.png" alt="" class="img-center image-responsive"/>
                    <p>
                        As the cosmic odyssey unfolds, the denouement reveals a pivotal stage—the exploitation of 
                        newfound wisdom. The tool-use examples, forged in the crucible of exploration, become the elixir 
                        that fine-tunes the LLM. Alternatively, the ancient scrolls of In-Context Learning (ICL) unfold, 
                        allowing the retrieval of examples from the hallowed trials of exploration. In this mystical realm, 
                        the seekers of knowledge find themselves at the crossroads of imagination and reality, wielding the 
                        enchanted tools to script their destiny.
                    </p>
                    
                </h4>
                
            </div>
        </div>
        <div class="row">
        
            <div class="col-md-12">
                <h2 class="head-last text-justify"><b> ToolBench APIs</b></h2>
                <h4 class="head-last text-justify">
                    <p>
                        Amidst the celestial crucible of ToolBench APIs, a fellowship of visionary researchers, 
                        including the illustrious <a href="https://arxiv.org/pdf/2403.04746.pdf">Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, and Yu Su</a>, 
                        embarked on an odyssey that unraveled profound revelations:
                    </p><p>
                        In the enchanted realm of Large Language Models (LLMs), a lament echoed—their 
                        tool-use prowess, a mere flicker in the cosmic expanse. GPT-4 stood adorned with a 
                        correctness veil of 60.8%, and the specialized artisan, ToolLLaMAv2, a mere luminary 
                        at 37.3%.
                    </p>
                    <p>
                        A mystical incantation, Simulated Trial and Error (STE), resonated through the hallowed
                         halls of discovery, proving to be the elixir that bestowed upon LLMs an arcane 
                         augmentation in both the sacred rites of In-Context Learning (ICL) and the refined 
                         dance of fine-tuning. Mistral-Instruct-7B, once a mere apprentice, ascended to glory 
                         with a triumphant 76.8% correctness—a surge of 46.7%, eclipsing even the might of 
                         GPT-4 with ICL.
                    </p>
                    <p>
                        Yet, the saga unveiled a quandary—a cosmic conundrum in the continual introduction of 
                        new tools. Fine-tuning, a double-edged sword, presented the specter of catastrophic 
                        forgetting, threatening to cast a shadow over the hallowed knowledge of existing 
                        tools. Fear not, for the researchers, architects of destiny, wielded a 
                        weapon—Experience Replay Strategy. This enchantment wove a protective cloak, 
                        preserving the ancient skills while ushering in the wisdom of new tools.
                    </p>
                    <p>
                        In the epic's denouement, inspired by the timeless dance of tool mastery 
                        among mortals, the researchers unfurled Simulated Trial and Error (STE)—a sacred rite 
                        for Large Language Models (LLMs) to commune with the tools of creation. 
                        This arcane art, born of a progressive memory-based trial-and-error framework, 
                        manifested its prowess through experiments with ToolBench APIs. As the cosmic dust 
                        settled, the echoes of a rehearsal-based fine-tuning hymn reverberated—a melody that 
                        harmonized the continuous learning of new tools while cradling the cherished skills of 
                        yore. The researchers, torchbearers of wisdom, thus inscribed their saga in the annals
                         of scholarly constellations.
                    </p>
                </h4>
                <!-- a href="#about" ><img src="assets/img/logo.png" alt="" class="img-center image-responsive"/></a -->
            </div>
            <div class="col-md-6 col-md-offset-3 slide-custom">
                <img src="/assets/img/blog/tools-LLM-4.png" alt="" class="img-c"  />
                    
            </div>
        </div>
        <div class="row">
        
            <div class="col-md-12">
                <h2 class="head-last text-justify"><b>The epic saga of exploration and mastery</b></h2>
                <h4 class="head-last text-justify">
                    <p>
                        In the epic saga of exploration and mastery, the authors, akin to wise sages, 
                        acknowledged certain constraints in their noble quest:
                    </p>
                    <p>
                        <b>Iterative Enhancement:</b> The current saga unfolds with the deployment of stalwart 
                        models for the perilous journey of exploration, while relying on diminutive models of 
                        lesser prowess for the exploits that follow. Yet, a whisper in the cosmic winds 
                        suggests an alternative path—a journey of iterative exploration and exploitation, a 
                        concept etched in the ancient scrolls of previous studies. In this unfolding tale, 
                        the once unyielding dependence on mighty models might gradually wane, perhaps casting 
                        them into the role of vigilant sentinels as the burgeoning capabilities of the enhanced
                         models unfurl over the sands of time.
                    </p>
                    <p>
                        <b>Compositional Symphony of Tools & Stratagems: </b> Another chapter in the grand 
                        tapestry of toolcraft unveils the need for the orchestration of multiple tools, 
                        a symphony of calls to address the complex queries that echo through the corridors 
                        of knowledge. Alas, this endeavor diverges from the authors' current focus, a 
                        departure noted with an air of contemplation. Recent scrolls foretell a 
                        revelation—Large Language Models (LLMs), beings of intrinsic ability, may channel 
                        their fundamental powers from the epochs of pretraining without the elaborate rituals 
                        of extensive fine-tuning or alignment. Thus, the cosmic forces suggest that, contrary 
                        to the emphasis on prodigious learning and boundless exploration, the adaptation of 
                        LLMs for the intricate dance of toolcraft may require not the forging of extensive 
                        data, but a communion with the wisdom from the tool side.
                    </p>
                    <p>
                        <b>The Boundless Memory Odyssey: </b> Within the ethereal corridors of remembrance, 
                        the authors acknowledge a constraint—the augmented memory, a sanctuary of knowledge, 
                        bound by the limits of the Large Language Model's (LLM) contextual grasp. To unravel 
                        this enigma, the authors invoke the cosmic musings, contemplating avenues to expand 
                        this sanctum of memory. Whispers speak of arcane techniques—adding retrieval modules 
                        as mystical keys or sculpting the memory into hierarchies and 
                        compressions. These paths, illuminated by the cosmic constellations, beckon the 
                        authors to transcend the limits and forge a memory that echoes through the ages.
                    </p>
                    <p>
                        <b>Tool Oblivion and the Unlearning Quest:</b> In the enchanting chronicles of 
                        discovery, while the authors embarked on a noble odyssey of ceaseless learning, 
                        a formidable nemesis emerged—Tool Unlearning. This adversary, born from the constant 
                        shifting tides of tools unloading and fading into the mists of obsolescence, cast a 
                        shadow over the heroes' path. The complexity of untangling the tendrils of acquired 
                        knowledge, a known enigma whispered in the ancient scrolls, now demanded attention. 
                        Yet, hope gleamed on the horizon, and the authors glimpsed a beacon in the form of 
                        <a href="https://arxiv.org/abs/2305.11554"> ToolkenGPT</a>. This legendary artifact promised a plug-and-play adaptation, a mystical
                        key to the labyrinth of unlearning, while drawing inspiration from the vast tapestry
                         of large-scale examples.
                    </p>

                    <p>
                        <b>Boundaries of Example-Led Refinement:</b> As the heroes navigated the uncharted 
                        territories of example-based fine-tuning, they encountered an elusive challenge—the 
                        delineation between when to wield a tool and when to restrain. The arcane art of 
                        instructing the model to discern this delicate balance, using positive tool-use 
                        examples alone, proved to be a conundrum etched in the cosmic code. The authors, 
                        akin to astute seers, foresaw several potential pathways to pierce through this veil 
                        of uncertainty. These included invoking the shadows of negative examples, with the 
                        contrast of objectives as their guiding light. Alternatively, weaving the intricate 
                        threads of API aspects into the fabric of example-based training became another secret
                         incantation. The authors, in their wisdom, left these mystical avenues open for the 
                         future scribes and scholars to explore, knowing that the quest for refinement in the 
                         realm of toolcraft is an ever-evolving saga.
                    </p>
                    <h2 class="head-last text-justify"><a href="https://github.com/microsoft/simulated-trial-and-error">Ref.: See on Github</a></h2>
                </h4>
                <!-- a href="#about" ><img src="assets/img/logo.png" alt="" class="img-center image-responsive"/></a -->
            </div>
        </div>
        
    </div>
</div>
    <!--END HOME SECTION-->
    
    <!--FOOTER SECTION -->
    <div class="for-full-back" id="footer">
        © 2024 www.solidreturn.dev 
         
    </div>
    <!-- END FOOTER SECTION -->

    <!-- JAVASCRIPT FILES PLACED AT THE BOTTOM TO REDUCE THE LOADING TIME  -->
    <!-- CORE JQUERY  -->
    <!-- >script src="assets/plugins/jquery-1.10.2.js"></script -->
    <script src="https://code.jquery.com/jquery-2.1.3.min.js"></script>
    <!-- BOOTSTRAP CORE SCRIPT   -->
    <script src="/assets/plugins/bootstrap.js"></script>  
    <!-- VEGAS SLIDESHOW SCRIPTS -->
    <!-- >script src="assets/plugins/vegas/jquery.vegas.min.js"></script-->
    <script src="https://jaysalvat.github.io/vegas/releases/latest/vegas.min.js"></script>
     <!-- SCROLL SCRIPTS -->
    <script src="/assets/plugins/jquery.easing.min.js"></script>
    <!-- CUSTOM SCRIPTS -->
    <script src="/assets/js/custom.js"></script>

</body>
</html>
